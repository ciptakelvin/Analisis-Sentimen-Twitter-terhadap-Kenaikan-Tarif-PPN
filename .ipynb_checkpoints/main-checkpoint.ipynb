{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75e42ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "#Import Classifier Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "class Preprocessor():\n",
    "    def __init__(self,text):\n",
    "        \"\"\"\n",
    "        Preprocessing Text: Digunakan untuk membersihkan teks sebelum dilakukan analisis.\n",
    "        mencakup proses casefolding, filtering\n",
    "        \"\"\"\n",
    "        self.text=text\n",
    "        self._casefolding()\n",
    "        self._filtering()\n",
    "        self._tokenize()\n",
    "        self._standarize()\n",
    "        self._stemming()\n",
    "\n",
    "    def get_text(self):\n",
    "        return \" \".join(self.text)\n",
    "    \n",
    "    def _casefolding(self):\n",
    "        #Mengubah menjadi huruf kecil        \n",
    "        self.text=self.text.lower()\n",
    "    \n",
    "    def _filtering(self):        \n",
    "        #Url\n",
    "        self.text=re.sub(\"https\\S+\",\"\",self.text)\n",
    "        self.text=re.sub(\"http\\S+\",\"\",self.text)\n",
    "        self.text=re.sub(\"\\S+\\.com\\S+\",\"\",self.text)\n",
    "        self.text=re.sub(\"\\S+\\.com\",\"\",self.text)\n",
    "        \n",
    "        #Remove Hashtag\n",
    "        self.text=re.sub(\"#\\S+\",\"\",self.text)\n",
    "        \n",
    "        #Remove Mention\n",
    "        self.text=re.sub(\"@\\S+\",\"\",self.text)\n",
    "        \n",
    "        #Remove Symbol and Number\n",
    "        self.text=re.sub(\"[^A-Za-z\\s]\",\" \",self.text)\n",
    "        \n",
    "        #Remove Spacing\n",
    "        self.text=re.sub(\"\\s+\",\" \",self.text)\n",
    "        self.text=re.sub(\"^\\s\",\"\",self.text)\n",
    "        self.text=self.text\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        #Membagi kata\n",
    "        self.text=word_tokenize(self.text)\n",
    "\n",
    "    def _standarize(self):        \n",
    "        #Mengubah menjadi kata baku\n",
    "        j={}\n",
    "        with open(\"standard_word.csv\",\"r\") as file:\n",
    "            data=csv.reader(file,delimiter=\",\")\n",
    "            for k,i in enumerate(data):\n",
    "                if k==0: continue\n",
    "                j[i[0]]=i[1]\n",
    "                \n",
    "        for k,t in enumerate(self.text):\n",
    "            if t in j:\n",
    "                self.text[k]=j[t]\n",
    "    \n",
    "    def _stemming(self):\n",
    "        #Mengubah menjadi kata dasar\n",
    "        factory=StemmerFactory()\n",
    "        stemmer=factory.create_stemmer()\n",
    "        \n",
    "        for k,i in enumerate(self.text):\n",
    "            self.text[k]=stemmer.stem(i)\n",
    "    \n",
    "class Analyzer():  \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Membuat model dan melakukan prediksi\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict_by_model(self,model,data:pd.DataFrame):\n",
    "\n",
    "        #Output Data\n",
    "        target_column:int=len(data.columns)-1\n",
    "        X=data.iloc[:,data.columns!=data.columns[target_column]]\n",
    "        y=data[data.columns[target_column]]\n",
    "        prediction=model.predict(X)\n",
    "        return prediction\n",
    "    \n",
    "        \n",
    "    def create_model(self,data:pd.DataFrame):\n",
    "        target_column:int=len(data.columns)-1\n",
    "        X=data.iloc[:,data.columns!=data.columns[target_column]]\n",
    "        y=data[data.columns[target_column]]\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "        \n",
    "        models_used=[\n",
    "            KNeighborsClassifier(),\n",
    "            SVC(),\n",
    "            GaussianNB(),\n",
    "            MultinomialNB(),\n",
    "            DecisionTreeClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            GradientBoostingClassifier(),\n",
    "        ]\n",
    "        \n",
    "        max_accuracy=0\n",
    "        for i in models_used:\n",
    "            i.fit(X_train,y_train)\n",
    "            prediction=i.predict(X_test)\n",
    "            accuracy=accuracy_score(prediction,y_test)\n",
    "            \n",
    "            if accuracy>max_accuracy:\n",
    "                max_accuracy=accuracy\n",
    "                model=i\n",
    "            print(i,prediction)\n",
    "            print(\"Accuracy Score:\",accuracy,\"\\n\")\n",
    "        \n",
    "        print(\"Model saved using \"+model.__class__.__name__+\" model. Accuracy: \"+str(max_accuracy))\n",
    "        dump(model,\"models/\"+model.__class__.__name__+\" \"+str(datetime.now()).replace(\":\",\"\")+\".joblib\")\n",
    "\n",
    "def check_tweet(order):\n",
    "    tweet=data[\"Tweet\"][order]\n",
    "    pre=Preprocessor(tweet)\n",
    "    return \"Tweet order: \"+str(order)+\" Tweet: \"+pre.get_text()\n",
    "\n",
    "def check_random_tweet():\n",
    "    order=random.randint(0,len(data[\"Tweet\"])-1)\n",
    "    tweet=data[\"Tweet\"][order]\n",
    "    pre=Preprocessor(tweet)\n",
    "    return \"Tweet order: \"+str(order)+\" Tweet: \"+pre.get_text()\n",
    "\n",
    "# #Load Data\n",
    "full=pd.read_csv('data/clean.csv') #Full Data\n",
    "full[\"Label\"]=0\n",
    "labeled=pd.read_csv('data/labeled.csv') #Labeled Data\n",
    "\n",
    "#Select Data to Analyze\n",
    "df=labeled[labeled[\"Label\"].notnull()]\n",
    "ana_df=df.iloc[:,[6,7,8,9,10,20,21,27]]\n",
    "\n",
    "#Create Best Model\n",
    "ana=Analyzer()\n",
    "ana.create_model(ana_df)\n",
    "\n",
    "#Load Model\n",
    "# model=load('models/SVC 2023-04-03 215442.101498.joblib')\n",
    "# ana=Analyzer()\n",
    "# ana.predict_by_model(model,full.iloc[:,[6,7,8,9,10,20,21,27]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
